base: &base

  # Model config
  embed_dim: 384
  depth: 12
  dropout: 0
  patch_size: 8

  # Training config
  img_size: [360, 720]
  dt: 1 
  global_batch_size: 256 # number of samples per training batch
  num_epochs: 60
  amp_mode: none
  enable_apex: False
  enable_jit: False
  expdir: '/logs'
  lr_schedule: 'cosine'
  lr: 5E-4

  # Data
  data_loader_config: 'pytorch'
  num_data_workers: 2 # number of dataloader worker threads per proc
  n_in_channels: 20
  n_out_channels: 20
  train_data_path:   '/data/train'
  valid_data_path:   '/data/valid'
  inf_data_path:     '/data/test'
  time_means_path:   '/data/stats/time_means.npy'
  global_means_path: '/data/stats/global_means.npy'
  global_stds_path:  '/data/stats/global_stds.npy'

  # Comms
  wireup_info: env
  wireup_store: tcp

short_noopt:
  <<: *base
  num_epochs: 10
  num_data_workers: 0
  global_batch_size: 64

short: &short
  <<: *base
  num_epochs: 10
  num_data_workers: 8
  global_batch_size: 64

# Short config with full optimizations
short_opt:
  <<: *short
  data_loader_config: 'dali'
  num_data_workers: 8
  global_batch_size: 1

long_opt:
  <<: *short
  embed_dim: 1024
  data_loader_config: 'dali'
  num_data_workers: 8
  global_batch_size: 64

short_mp:
  <<: *short
  num_data_workers: 8
  global_batch_size: 64
